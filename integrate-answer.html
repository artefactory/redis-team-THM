<!DOCTYPE html>
<html lang="en">
<head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <meta name="generator" content="Pelican" />
        <title>Question Answering Integration</title>
        <link rel="stylesheet" href="https://artefactory.github.io/redis-team-THM/theme/css/main.css" />
        <meta name="description" content="Integrating a question answering pipeline into our workflow" />
</head>

<body id="index" class="home">
        <header id="banner" class="body">
                <h1><a href="https://artefactory.github.io/redis-team-THM/">Team THM</a></h1>
                <nav><ul>
                    <li class="active"><a href="https://artefactory.github.io/redis-team-THM/category/machine-learning.html">Machine Learning</a></li>
                    <li><a href="https://artefactory.github.io/redis-team-THM/category/mlops.html">MLOps</a></li>
                    <li><a href="https://artefactory.github.io/redis-team-THM/category/redisearch.html">RediSearch</a></li>
                    <li><a href="https://artefactory.github.io/redis-team-THM/category/team.html">Team</a></li>
                    <li><a href="https://artefactory.github.io/redis-team-THM/category/thm-cli.html">THM CLI</a></li>
                    <li><a href="https://artefactory.github.io/redis-team-THM/category/thm-indexing.html">THM Indexing</a></li>
                </ul></nav>
        </header><!-- /#banner -->
<section id="content" class="body">
  <article>
    <header>
      <h1 class="entry-title">
        <a href="https://artefactory.github.io/redis-team-THM/integrate-answer.html" rel="bookmark"
           title="Permalink to Question Answering Integration">Question Answering Integration</a></h1>
    </header>

    <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2022-11-03T15:28:00+01:00">
                Published: Thu 03 November 2022
        </abbr>
		<br />
        <abbr class="modified" title="2022-11-03T15:28:00+01:00">
                Updated: Thu 03 November 2022
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="https://artefactory.github.io/redis-team-THM/author/henrique-brito.html">Henrique Brito</a>
        </address>
<p>In <a href="https://artefactory.github.io/redis-team-THM/category/machine-learning.html">Machine Learning</a>.</p>
<p>tags: <a href="https://artefactory.github.io/redis-team-THM/tag/question-answering.html">question-answering</a> <a href="https://artefactory.github.io/redis-team-THM/tag/huggingface.html">huggingface</a> <a href="https://artefactory.github.io/redis-team-THM/tag/transformers.html">transformers</a> </p>
</footer><!-- /.post-info -->      <p><em>Day 10 - Integrating the question answering approach into our workflow</em></p>
<h1>Hugging Face pipeline</h1>
<p>In the <a href="https://artefactory.github.io/redis-team-THM/find-answer.html">last post</a> we had decided to implement a Question Answering <a href="https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.QuestionAnsweringPipeline:~:text=passed%20as%20inputs.-,QuestionAnsweringPipeline,-class%20transformers.">pipeline</a> from Huggingface to handle open-ended questions from users. After an experimentation phase performed on a notebook, we had to industrialize our code. Since the <strong>question</strong> input by the user has to be embedded with the <strong>context</strong> (in this case the papers titles and abstracts), we could not preemptively store the embeddings on Redis. This means that the embeddings have to be done in real-time when the user makes a question using the CLI. We implemented this logic in our <a href="https://github.com/artefactory/redis-team-THM/blob/main/scripts/question_answering.py">code</a> using a function that we integrated in the CLI.</p>
<h1>Leveraging Redis to speed up inference</h1>
<p>One problem with this approach is that the embedding of large corpora can take a long time. If we want to look for the answer to a question among the entire arXiv dataset, we would need to encode all titles and abstracts every time the user submits a query. This would render the application impossible to use. In order to speed up the real-time inference we relied on RediSearch to select a subset of priority papers that are most related to the users question. We make an API call to the endpoint <code>/vectorsearch/text/user</code> from our <a href="https://artefactory.github.io/redis-team-THM/services.html">backend</a> and retrieve a list of papers that are most related to the question using the similarity between their embeddings. Once we have this list, we apply the pipeline to the corpora of all the texts combined and extract a response to the user's answer.</p>
<h1>Results</h1>
<p>We found that this feature showed interesting results for a first iteration. In many cases it could provide relevant answers to the user's questions:</p>
<div class="highlight"><pre><span></span><code><span class="c">Ask what is on your mind: what was stephen hawking most brilliant discovery?</span>
<span class="nb">--------------------------------------------------------------------------------</span><span class="c"></span>
<span class="c">Answer: &#39;black holes have an entropy and consequently a finite temperature&#39;</span>
</code></pre></div>

<p>It manages to find names of domain-specific names of algorithms:</p>
<div class="highlight"><pre><span></span><code><span class="c">Ask what is on your mind: what is the best algorithm to multiply big matrix</span>
<span class="nb">--------------------------------------------------------------------------------</span><span class="c"></span>
<span class="c">Answer: &#39;tSparse&#39;</span>
</code></pre></div>

<p>And even reply to open-ended questions:</p>
<div class="highlight"><pre><span></span><code><span class="c">Ask what is on your mind: what are the challenges of climate change</span>
<span class="nb">--------------------------------------------------------------------------------</span><span class="c"></span>
<span class="c">Answer: &#39;reducing inequalities and responsible consumption&#39;</span>
</code></pre></div>

<p>Even when the answer itself is not 100% pertinent, the paper where it came from can be a good reading suggestion</p>
<div class="highlight"><pre><span></span><code>Ask what is on your mind: How to perform assortment optimization?
--------------------------------------------------------------------------------
Answer: &#39;using a computer algebra system&#39;

This answer came from here:
A Computational Approach to Essential and Nonessential Objective Functions in Linear Multicriteria Optimization
by Agnieszka B. Malinowska, Delfim F. M. Torres
</code></pre></div>

<p>In other cases, the model seems to miss the point of the question:</p>
<div class="highlight"><pre><span></span><code><span class="c">Ask what is on your mind: What is machine learning?</span>
<span class="nb">--------------------------------------------------------------------------------</span><span class="c"></span>
<span class="c">Answer: &#39;artificial intelligence&#39;</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="c">Ask what is on your mind: how many colors in needed to color a map</span>
<span class="nb">--------------------------------------------------------------------------------</span><span class="c"></span>
<span class="c">Answer: &#39;6992 and 24&#39;</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="c">Ask what is on your mind: Can computer beat humans at the game of go?</span>
<span class="nb">--------------------------------------------------------------------------------</span><span class="c"></span>
<span class="c">Answer: &#39;None of those agents succeeds in beating our agent&#39;</span>
</code></pre></div>

<p>But most importantly, the results provided by this feature are seldom contained in the results of the vector search. This is encouraging as it increases the potential for serendipity of our global solution.</p>
<h1>Future improvements</h1>
<p>For this hackathon we used a pre-trained question answering model from Huggingface. The model we employed has been trained on a data from a large range of domains. This make so that scientific terms, which are not very common outside of the scientific community end up having a vector representation that is not nuanced enough to guarantee great performances. This problem could be mitigated by fine-tuning a question answering model with scientific questions. The biggest challenge in doing so is that these models require training sets with pre-annotated questions ans answer, which is very hard to achieve programmatically and would most likely have to done manually.</p>
<p>Another problem with this approach is that, despite the optimizations brought by using Redis to choose priority papers, the inference time is still relatively long. The model currently takes about 45s to output answer. In the future, it would probably be best to migrate our technical stack to a framework such as <a href="https://haystack.deepset.ai/overview/intro">haystack</a> instead of Huggingface. In addition to that, we could envisage deploying our model and making it available via an endpoint, instead of performing the calculations locally. This type of deployment come with significant security considerations, however, as the serving is usually done with the use of GPUs.</p>
    </div><!-- /.entry-content -->

  </article>
</section>
        <section id="extras" class="body">
                <div class="blogroll">
                        <h2>links</h2>
                        <ul>
                            <li><a href="https://github.com/artefactory/redis-team-THM/">GitHub</a></li>
                            <li><a href="https://www.artefact.com">Artefact</a></li>
                            <li><a href="https://redis.io/docs/stack">Redis Ventures</a></li>
                        </ul>
                </div><!-- /.blogroll -->
        </section><!-- /#extras -->

        <footer id="contentinfo" class="body">
                <address id="about" class="vcard body">
                Proudly powered by <a href="https://getpelican.com/">Pelican</a>, which takes great advantage of <a href="https://www.python.org/">Python</a>.
                </address><!-- /#about -->

                <p>The theme is by <a href="https://www.smashingmagazine.com/2009/08/designing-a-html-5-layout-from-scratch/">Smashing Magazine</a>, thanks!</p>
        </footer><!-- /#contentinfo -->

    <script type="text/javascript">
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'G-H5TDMDHN41', 'auto');
    ga('send', 'pageview');
    </script>
</body>
</html>