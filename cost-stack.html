<!DOCTYPE html>
<html lang="en">
<head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <meta name="generator" content="Pelican" />
        <title>Benchmarks</title>
        <link rel="stylesheet" href="https://artefactory.github.io/redis-team-THM/theme/css/main.css" />
        <meta name="description" content="Benchmarks" />
</head>

<body id="index" class="home">
        <header id="banner" class="body">
                <h1><a href="https://artefactory.github.io/redis-team-THM/">Team THM</a></h1>
                <nav><ul>
                    <li><a href="https://artefactory.github.io/redis-team-THM/category/machine-learning.html">Machine Learning</a></li>
                    <li class="active"><a href="https://artefactory.github.io/redis-team-THM/category/mlops.html">MLOps</a></li>
                    <li><a href="https://artefactory.github.io/redis-team-THM/category/redisearch.html">RediSearch</a></li>
                    <li><a href="https://artefactory.github.io/redis-team-THM/category/team.html">Team</a></li>
                    <li><a href="https://artefactory.github.io/redis-team-THM/category/thm-cli.html">THM CLI</a></li>
                    <li><a href="https://artefactory.github.io/redis-team-THM/category/thm-indexing.html">THM Indexing</a></li>
                </ul></nav>
        </header><!-- /#banner -->
<section id="content" class="body">
  <article>
    <header>
      <h1 class="entry-title">
        <a href="https://artefactory.github.io/redis-team-THM/cost-stack.html" rel="bookmark"
           title="Permalink to Benchmarks">Benchmarks</a></h1>
    </header>

    <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2022-10-30T12:05:00+01:00">
                Published: Sun 30 October 2022
        </abbr>
		<br />
        <abbr class="modified" title="2022-10-30T12:05:00+01:00">
                Updated: Sun 30 October 2022
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="https://artefactory.github.io/redis-team-THM/author/michel-hua.html">Michel Hua</a>
        </address>
<p>In <a href="https://artefactory.github.io/redis-team-THM/category/mlops.html">MLOps</a>.</p>
<p>tags: <a href="https://artefactory.github.io/redis-team-THM/tag/redis.html">redis</a> </p>
</footer><!-- /.post-info -->      <p><em>Day 8 - When we ran some benchmark for the process of index creation and hosting.</em></p>
<h1>Benchmarking the loading scripts</h1>
<p>From a file named <code>arxiv-metadata-oai-snapshot.json</code> containing metadata and abstracts of aout 2M papers in 153 different scientific categories, we generated partial indexes and evaluated how it can run in production considering:</p>
<ul>
<li>machine provisining needed,</li>
<li>how to update regularly from the arXiv snapshots,</li>
<li>volumetry of data.</li>
</ul>
<h2>Generating Embeddings</h2>
<p>First, we evaluated the Jupyter notebooks from Redis demo code <a href="https://github.com/RedisVentures/redis-arXiv-search/tree/main/data"><code>RedisVentures/redis-arXiv-search</code></a>.</p>
<table>
<thead>
<tr>
<th style="text-align: right;">Model</th>
<th>Machine</th>
<th style="text-align: right;">Time</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: right;"><code>arxiv-embeddings.ipynb</code></td>
<td><a href="https://www.apple.com/macbook-pro-14-and-16/specs/">Apple M1 Pro 8-core</a></td>
<td style="text-align: right;">17min</td>
</tr>
<tr>
<td style="text-align: right;"><code>arxiv-embeddings.ipynb</code></td>
<td><a href="https://saturncloud.io/plans/hosted/">Saturn Cloud T4-XLarge 4-cores</a></td>
<td style="text-align: right;">4min</td>
</tr>
<tr>
<td style="text-align: right;"><code>single-gpu-arxiv-embeddings.ipynb</code></td>
<td>T4-XLarge 4-cores, <code>saturn-python-rapids</code> image</td>
<td style="text-align: right;">30min</td>
</tr>
</tbody>
</table>
<p>To then load the Pickle index to Redis is easy from a normal desktop machine.</p>
<table>
<thead>
<tr>
<th style="text-align: right;">Model</th>
<th>Machine</th>
<th style="text-align: right;">Time</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: right;"><code>arxiv_embeddings_10000.pkl</code></td>
<td><a href="https://www.apple.com/macbook-pro-14-and-16/specs/">Apple M1 Pro 8-core</a></td>
<td style="text-align: right;">6min</td>
</tr>
</tbody>
</table>
<h2>Profiling the code</h2>
<p>After refactoring to Python scripts, we used <a href="https://github.com/Delgan/loguru"><code>Delgan/loguru</code></a> to add logs, <a href="https://github.com/tqdm/tqdm"><code>tqdm/tqdm</code></a> to track troughput and <a href="https://github.com/pythonprofilers/memory_profiler"><code>pythonprofilers/memory_profiler</code></a> to find pain points and we measured that,</p>
<p>For <code>generate_index.py</code> script the <code>model.encode(sentence)</code> was the largest pain point, and finding optimal way to do it was important.</p>
<div class="highlight"><pre><span></span><code>$ python3 -m memory_profiler generate_index.py <span class="se">\</span>
  --year_month<span class="o">=</span><span class="m">200811</span> <span class="se">\</span>
  --input_path<span class="o">=</span><span class="s2">&quot;arxiv-metadata-oai-snapshot.json&quot;</span> <span class="se">\</span>
  --output_path<span class="o">=</span><span class="s2">&quot;arxiv/cutoff=200811/output.pkl&quot;</span> <span class="se">\</span>
  --model_name<span class="o">=</span><span class="s2">&quot;sentence-transformers/all-MiniLM-L12-v2&quot;</span>

Line <span class="c1">#    Mem usage    Increment  Occurrences   Line Contents</span>
<span class="o">=============================================================</span>
    <span class="m">33</span>    <span class="m">292</span>.0 MiB    <span class="m">292</span>.0 MiB           <span class="m">1</span>   @profile
    <span class="m">34</span>                                         def run<span class="o">(</span>
    <span class="m">35</span>                                             year_month,
    <span class="m">36</span>                                             <span class="nv">input_path</span><span class="o">=</span><span class="s2">&quot;arxiv-metadata-oai-snapshot.json&quot;</span>,
    <span class="m">37</span>                                             <span class="nv">output_path</span><span class="o">=</span><span class="s2">&quot;arxiv_embeddings_10000.pkl&quot;</span>,
    <span class="m">38</span>                                             <span class="nv">model_name</span><span class="o">=</span><span class="s2">&quot;sentence-transformers/all-mpnet-base-v2&quot;</span>,
    <span class="m">39</span>                                         <span class="o">)</span>:
    <span class="m">40</span>                                             <span class="s2">&quot;&quot;&quot;Generate Embeddings and Create a File Index.&quot;&quot;&quot;</span>
    <span class="m">41</span>
    <span class="m">42</span>    <span class="m">292</span>.0 MiB      <span class="m">0</span>.0 MiB           <span class="m">1</span>       logger.info<span class="o">(</span>f<span class="s2">&quot;Reading papers for {year_month}...&quot;</span><span class="o">)</span>
    <span class="m">43</span>    <span class="m">315</span>.4 MiB     <span class="m">23</span>.4 MiB           <span class="m">1</span>       <span class="nv">df</span> <span class="o">=</span> pd.DataFrame<span class="o">(</span>get_papers<span class="o">(</span>input_path, year_month<span class="o">))</span>
    <span class="m">44</span>
    <span class="m">45</span>    <span class="m">315</span>.9 MiB      <span class="m">0</span>.4 MiB           <span class="m">1</span>       logger.info<span class="o">(</span><span class="s2">&quot;Getting categories predictions&quot;</span><span class="o">)</span>
    <span class="m">46</span>                                             <span class="c1"># df[&quot;predicted_categories&quot;] = get_paper_classification_predictions(</span>
    <span class="m">47</span>                                             <span class="c1">#     df[&quot;title&quot;] + &quot; &quot; + df[&quot;abstract&quot;], top_k=3</span>
    <span class="m">48</span>                                             <span class="c1"># )</span>
    <span class="m">49</span>
    <span class="m">50</span>                                             <span class="c1"># https://www.sbert.net/docs/usage/semantic_textual_similarity.html</span>
    <span class="m">51</span>    <span class="m">447</span>.0 MiB    <span class="m">131</span>.1 MiB           <span class="m">1</span>       <span class="nv">model</span> <span class="o">=</span> SentenceTransformer<span class="o">(</span>model_name<span class="o">)</span>
    <span class="m">52</span>
    <span class="m">53</span>    <span class="m">447</span>.0 MiB      <span class="m">0</span>.0 MiB           <span class="m">1</span>       logger.info<span class="o">(</span><span class="s2">&quot;Creating embeddings from title and abstract...&quot;</span><span class="o">)</span>
    <span class="m">54</span>    <span class="m">447</span>.0 MiB      <span class="m">0</span>.0 MiB           <span class="m">1</span>       logger.info<span class="o">(</span>model_name<span class="o">)</span>
    <span class="m">55</span>
    <span class="m">56</span>    <span class="m">724</span>.2 MiB    <span class="m">103</span>.8 MiB           <span class="m">2</span>       df<span class="o">[</span><span class="s2">&quot;vector&quot;</span><span class="o">]</span> <span class="o">=</span> df.progress_apply<span class="o">(</span>
    <span class="m">57</span>    <span class="m">620</span>.4 MiB -3613776.1 MiB       <span class="m">59149</span>           lambda x: _featurize<span class="o">(</span>model, x<span class="o">[</span><span class="s2">&quot;title&quot;</span><span class="o">]</span>, x<span class="o">[</span><span class="s2">&quot;abstract&quot;</span><span class="o">])</span>, <span class="nv">axis</span><span class="o">=</span><span class="m">1</span>
    <span class="m">58</span>                                             <span class="o">)</span>
    <span class="m">59</span>    <span class="m">729</span>.7 MiB      <span class="m">5</span>.5 MiB           <span class="m">1</span>       <span class="nv">df</span> <span class="o">=</span> df.reset_index<span class="o">()</span>.drop<span class="o">(</span><span class="s2">&quot;index&quot;</span>, <span class="nv">axis</span><span class="o">=</span><span class="m">1</span><span class="o">)</span>
    <span class="m">60</span>
    <span class="m">61</span>    <span class="m">730</span>.0 MiB      <span class="m">0</span>.3 MiB           <span class="m">1</span>       <span class="nv">df</span> <span class="o">=</span> df.reset_index<span class="o">()</span>.drop<span class="o">(</span><span class="s2">&quot;index&quot;</span>, <span class="nv">axis</span><span class="o">=</span><span class="m">1</span><span class="o">)</span>
    <span class="m">62</span>
    <span class="m">63</span>    <span class="m">731</span>.5 MiB      <span class="m">1</span>.5 MiB           <span class="m">1</span>       logger.info<span class="o">(</span><span class="s2">&quot;Exporting to pickle file...&quot;</span><span class="o">)</span>
    <span class="m">64</span>    <span class="m">731</span>.5 MiB      <span class="m">0</span>.0 MiB           <span class="m">1</span>       with open<span class="o">(</span>output_path, <span class="s2">&quot;wb&quot;</span><span class="o">)</span> as f:
    <span class="m">65</span>    <span class="m">917</span>.0 MiB    <span class="m">185</span>.5 MiB           <span class="m">1</span>           <span class="nv">data</span> <span class="o">=</span> pickle.dumps<span class="o">(</span>df<span class="o">)</span>
    <span class="m">66</span>    <span class="m">919</span>.8 MiB      <span class="m">2</span>.7 MiB           <span class="m">1</span>           f.write<span class="o">(</span>data<span class="o">)</span>
</code></pre></div>

<p>As we can see using <code>pandas</code> to load and store data wasn't optimal because of its memory overhead.</p>
<div class="highlight"><pre><span></span><code>$ python3 -m memory_profiler load_data.py <span class="se">\</span>
  --concurrency_level<span class="o">=</span><span class="m">2</span> <span class="se">\</span>
  --separator<span class="o">=</span><span class="s2">&quot;|&quot;</span>  <span class="se">\</span>
  --vector_size<span class="o">=</span><span class="m">384</span> <span class="se">\</span>
  --reset_db<span class="o">=</span>False <span class="se">\</span>
  --embeddings_path<span class="o">=</span><span class="s2">&quot;arxiv/cutoff=200811/output.pkl&quot;</span>

Line <span class="c1">#    Mem usage    Increment  Occurrences   Line Contents</span>
<span class="o">=============================================================</span>
    <span class="m">98</span>     <span class="m">67</span>.0 MiB     <span class="m">67</span>.0 MiB           <span class="m">1</span>   @profile
    <span class="m">99</span>                                         def run<span class="o">(</span>
   ...
   <span class="m">107</span>
   <span class="m">108</span>     <span class="m">67</span>.2 MiB      <span class="m">0</span>.2 MiB           <span class="m">1</span>       <span class="nv">config</span> <span class="o">=</span> get_settings<span class="o">()</span>
   <span class="m">109</span>
   <span class="m">110</span>     <span class="m">67</span>.2 MiB      <span class="m">0</span>.0 MiB           <span class="m">1</span>       <span class="k">if</span> reset_db:
   <span class="m">111</span>                                                 logger.info<span class="o">(</span>f<span class="s2">&quot;TODO {reset_db}&quot;</span><span class="o">)</span>
   <span class="m">112</span>
   <span class="m">113</span>     <span class="m">67</span>.2 MiB      <span class="m">0</span>.0 MiB           <span class="m">2</span>       Paper.Meta.database <span class="o">=</span> get_redis_connection<span class="o">(</span>
   <span class="m">114</span>     <span class="m">67</span>.2 MiB      <span class="m">0</span>.0 MiB           <span class="m">1</span>           <span class="nv">url</span><span class="o">=</span>config.get_redis_url<span class="o">()</span>, <span class="nv">decode_responses</span><span class="o">=</span>True
   <span class="m">115</span>                                             <span class="o">)</span>
   <span class="m">116</span>     <span class="m">67</span>.2 MiB      <span class="m">0</span>.0 MiB           <span class="m">1</span>       Paper.Meta.global_key_prefix <span class="o">=</span> <span class="s2">&quot;THM&quot;</span>
   <span class="m">117</span>     <span class="m">67</span>.2 MiB      <span class="m">0</span>.0 MiB           <span class="m">1</span>       Paper.Meta.model_key_prefix <span class="o">=</span> <span class="s2">&quot;Paper&quot;</span>
   <span class="m">118</span>
   <span class="m">119</span>     <span class="m">67</span>.2 MiB      <span class="m">0</span>.0 MiB           <span class="m">1</span>       <span class="nv">redis_conn</span> <span class="o">=</span> redis.from_url<span class="o">(</span>config.get_redis_url<span class="o">())</span>
   <span class="m">120</span>
   <span class="m">121</span>    <span class="m">165</span>.1 MiB     <span class="m">97</span>.8 MiB           <span class="m">2</span>       asyncio.run<span class="o">(</span>
   <span class="m">122</span>     <span class="m">67</span>.2 MiB      <span class="m">0</span>.0 MiB           <span class="m">2</span>           load_all_data<span class="o">(</span>
   ...


    <span class="m">51</span>   <span class="m">67</span>.969 MiB   <span class="m">67</span>.969 MiB           <span class="m">1</span>   @profile
    <span class="m">52</span>                                         async def load_all_data<span class="o">(</span>
   ...
    <span class="m">62</span>   <span class="m">67</span>.984 MiB    <span class="m">0</span>.016 MiB           <span class="m">1</span>       logger.info<span class="o">(</span><span class="s2">&quot;Loading papers...&quot;</span><span class="o">)</span>
    <span class="m">63</span>  <span class="m">607</span>.562 MiB  <span class="m">539</span>.578 MiB           <span class="m">1</span>       <span class="nv">papers</span> <span class="o">=</span> read_paper_df<span class="o">(</span>embeddings_path<span class="o">)</span>.head<span class="o">(</span><span class="m">1</span><span class="o">)</span>
    <span class="m">64</span>  <span class="m">152</span>.219 MiB -455.344 MiB           <span class="m">1</span>       <span class="nv">papers</span> <span class="o">=</span> papers.to_dict<span class="o">(</span><span class="s2">&quot;records&quot;</span><span class="o">)</span>
    <span class="m">65</span>
    <span class="m">66</span>  <span class="m">152</span>.219 MiB    <span class="m">0</span>.000 MiB           <span class="m">1</span>       logger.info<span class="o">(</span><span class="s2">&quot;Writing to Redis...&quot;</span><span class="o">)</span>
    <span class="m">67</span>  <span class="m">152</span>.578 MiB  <span class="m">304</span>.844 MiB           <span class="m">4</span>       await gather_with_concurrency<span class="o">(</span>
    <span class="m">68</span>  <span class="m">152</span>.219 MiB    <span class="m">0</span>.000 MiB           <span class="m">2</span>           redis_conn, concurrency_level, separator, vector_size, *papers
</code></pre></div>

<h1>Load testing the HTTP Server</h1>
<p>Using <a href="https://github.com/wg/wrk"><code>wg/wrk</code></a> we have made a few tests to see how much HTTP server and Redis could handle if many clients connected to it.</p>
<div class="highlight"><pre><span></span><code>$ wrk -t4 -c20 -d30s https://thm-cli.community.saturnenterprise.io/api/docs

Running 30s <span class="nb">test</span> @ https://thm-cli.community.saturnenterprise.io/api/docs
  <span class="m">4</span> threads and <span class="m">20</span> connections

  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency   <span class="m">105</span>.53ms   <span class="m">21</span>.50ms <span class="m">461</span>.06ms   <span class="m">88</span>.24%
    Req/Sec    <span class="m">47</span>.63      <span class="m">7</span>.97    <span class="m">80</span>.00     <span class="m">79</span>.85%
  <span class="m">5629</span> requests <span class="k">in</span> <span class="m">30</span>.11s, <span class="m">5</span>.74MB <span class="nb">read</span>
Requests/sec:    <span class="m">186</span>.98
Transfer/sec:    <span class="m">195</span>.19KB
</code></pre></div>

<div class="highlight"><pre><span></span><code>$ wrk -t4 -c10 -d30s --script benchmark/post_similar.lua https://thm-cli.community.saturnenterprise.io/api/v1/paper/vectorsearch/text
Running 30s <span class="nb">test</span> @ https://thm-cli.community.saturnenterprise.io/api/v1/paper/vectorsearch/text
  <span class="m">4</span> threads and <span class="m">10</span> connections
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency   <span class="m">352</span>.05ms   <span class="m">52</span>.12ms <span class="m">697</span>.77ms   <span class="m">89</span>.43%
    Req/Sec     <span class="m">5</span>.96      <span class="m">2</span>.66    <span class="m">10</span>.00     <span class="m">50</span>.36%
  <span class="m">672</span> requests <span class="k">in</span> <span class="m">30</span>.04s, <span class="m">12</span>.83MB <span class="nb">read</span>
Requests/sec:     <span class="m">22</span>.37
Transfer/sec:    <span class="m">437</span>.32KB
</code></pre></div>

<div class="highlight"><pre><span></span><code>$ wrk -t4 -c10 -d30s --script benchmark/post_text.lua https://thm-cli.community.saturnenterprise.io/api/v1/paper/vectorsearch/text/user
Running 30s <span class="nb">test</span> @ https://thm-cli.community.saturnenterprise.io/api/v1/paper/vectorsearch/text/user
  <span class="m">4</span> threads and <span class="m">10</span> connections
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency   <span class="m">575</span>.41ms  <span class="m">171</span>.71ms   <span class="m">1</span>.48s    <span class="m">89</span>.46%
    Req/Sec     <span class="m">4</span>.16      <span class="m">3</span>.14    <span class="m">10</span>.00     <span class="m">58</span>.23%
  <span class="m">408</span> requests <span class="k">in</span> <span class="m">30</span>.06s, <span class="m">8</span>.60MB <span class="nb">read</span>
Requests/sec:     <span class="m">13</span>.57
Transfer/sec:    <span class="m">292</span>.92KB
</code></pre></div>

<h1>Discussing the Cost of Hosting</h1>
<p>Pricing of the stack</p>
<ul>
<li>Redis Cloud Enterprise: $0.881/hr = USD 600/month</li>
<li>Saturn Cloud Notebooks and Deployment: $0.21/hour = USD 150/month</li>
</ul>
<p>The total is about 750 USD/month. You might be able to decrease costs a bit using infrastructure as a service instead of platform as a service, but more DevOps skills would be needed to configure cloud accounts on GCP or AWS for example.</p>
    </div><!-- /.entry-content -->

  </article>
</section>
        <section id="extras" class="body">
                <div class="blogroll">
                        <h2>links</h2>
                        <ul>
                            <li><a href="https://github.com/artefactory/redis-team-THM/">GitHub</a></li>
                            <li><a href="https://www.artefact.com">Artefact</a></li>
                            <li><a href="https://redisventures.com">Redis Ventures</a></li>
                        </ul>
                </div><!-- /.blogroll -->
        </section><!-- /#extras -->

        <footer id="contentinfo" class="body">
                <address id="about" class="vcard body">
                Proudly powered by <a href="https://getpelican.com/">Pelican</a>, which takes great advantage of <a href="https://www.python.org/">Python</a>.
                </address><!-- /#about -->

                <p>The theme is by <a href="https://www.smashingmagazine.com/2009/08/designing-a-html-5-layout-from-scratch/">Smashing Magazine</a>, thanks!</p>
        </footer><!-- /#contentinfo -->

</body>
</html>