{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Answering Workflow\n",
    "\n",
    "## Overview\n",
    "\n",
    "The objective of this notebook is to hae a workflows that allows us to suggest one paper to answer a specific question from a user. To this end, we will employ am extractive question answering approach. \n",
    "\n",
    "The workflow is separated into X different steps:\n",
    "\n",
    "* Validation of the query\n",
    "* Search of closest articles \n",
    "* Extraction of answer from article\n",
    "* Recovery of the source article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports & globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import pandas as pd\n",
    "\n",
    "import string\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../backend\")\n",
    "\n",
    "from thm.qa import models, paper_priority\n",
    "from arxiv_dataset import data_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_LOCATION = os.environ.get(\"DATA_LOCATION\", '/home/jovyan/arxiv/arxiv-metadata-oai-snapshot.json')\n",
    "DATA_LOCATION = '/home/jovyan/arxiv/arxiv-metadata-oai-snapshot.json'\n",
    "YEAR_CUTOFF = 2012\n",
    "ML_CATEGORY = \"cs.LG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def papers():\n",
    "    with open(DATA_LOCATION, 'r') as f:\n",
    "        for paper in f:\n",
    "            paper = data_load.parse_paper(paper)\n",
    "            if paper['year']:\n",
    "                if paper['year'] >= YEAR_CUTOFF and ML_CATEGORY in paper['categories']:\n",
    "                    yield paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_df = pd.DataFrame(papers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>authors</th>\n",
       "      <th>categories</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0705.4485</td>\n",
       "      <td>Mixed membership stochastic blockmodels</td>\n",
       "      <td>2014</td>\n",
       "      <td>Edoardo M Airoldi, David M Blei, Stephen E Fie...</td>\n",
       "      <td>stat.ME,cs.LG,math.ST,physics.soc-ph,stat.ML,s...</td>\n",
       "      <td>Observations consisting of measurements on r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0808.3231</td>\n",
       "      <td>Multi-Instance Multi-Label Learning</td>\n",
       "      <td>2012</td>\n",
       "      <td>Zhi-Hua Zhou, Min-Ling Zhang, Sheng-Jun Huang,...</td>\n",
       "      <td>cs.LG,cs.AI</td>\n",
       "      <td>In this paper, we propose the MIML (Multi-In...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0811.4413</td>\n",
       "      <td>A Spectral Algorithm for Learning Hidden Marko...</td>\n",
       "      <td>2012</td>\n",
       "      <td>Daniel Hsu, Sham M. Kakade, Tong Zhang</td>\n",
       "      <td>cs.LG,cs.AI</td>\n",
       "      <td>Hidden Markov Models (HMMs) are one of the m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0903.4817</td>\n",
       "      <td>An Exponential Lower Bound on the Complexity o...</td>\n",
       "      <td>2012</td>\n",
       "      <td>Bernd G\\\"artner, Martin Jaggi and Cl\\'ement Maria</td>\n",
       "      <td>cs.LG,cs.CG,cs.CV,math.OC,stat.ML</td>\n",
       "      <td>For a variety of regularized optimization pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0909.5175</td>\n",
       "      <td>Bounding the Sensitivity of Polynomial Thresho...</td>\n",
       "      <td>2013</td>\n",
       "      <td>Prahladh Harsha, Adam Klivans, Raghu Meka</td>\n",
       "      <td>cs.CC,cs.LG</td>\n",
       "      <td>We give the first non-trivial upper bounds o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              title  year  \\\n",
       "0  0705.4485            Mixed membership stochastic blockmodels  2014   \n",
       "1  0808.3231                Multi-Instance Multi-Label Learning  2012   \n",
       "2  0811.4413  A Spectral Algorithm for Learning Hidden Marko...  2012   \n",
       "3  0903.4817  An Exponential Lower Bound on the Complexity o...  2012   \n",
       "4  0909.5175  Bounding the Sensitivity of Polynomial Thresho...  2013   \n",
       "\n",
       "                                             authors  \\\n",
       "0  Edoardo M Airoldi, David M Blei, Stephen E Fie...   \n",
       "1  Zhi-Hua Zhou, Min-Ling Zhang, Sheng-Jun Huang,...   \n",
       "2             Daniel Hsu, Sham M. Kakade, Tong Zhang   \n",
       "3  Bernd G\\\"artner, Martin Jaggi and Cl\\'ement Maria   \n",
       "4          Prahladh Harsha, Adam Klivans, Raghu Meka   \n",
       "\n",
       "                                          categories  \\\n",
       "0  stat.ME,cs.LG,math.ST,physics.soc-ph,stat.ML,s...   \n",
       "1                                        cs.LG,cs.AI   \n",
       "2                                        cs.LG,cs.AI   \n",
       "3                  cs.LG,cs.CG,cs.CV,math.OC,stat.ML   \n",
       "4                                        cs.CC,cs.LG   \n",
       "\n",
       "                                            abstract  \n",
       "0    Observations consisting of measurements on r...  \n",
       "1    In this paper, we propose the MIML (Multi-In...  \n",
       "2    Hidden Markov Models (HMMs) are one of the m...  \n",
       "3    For a variety of regularized optimization pr...  \n",
       "4    We give the first non-trivial upper bounds o...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1104.4803\n",
      "Clustering Partially Observed Graphs via Convex Optimization\n",
      "2014\n",
      "Yudong Chen, Ali Jalali, Sujay Sanghavi and Huan Xu\n",
      "cs.LG,stat.ML\n",
      "  This paper considers the problem of clustering a partially observed\n",
      "unweighted graph---i.e., one where for some node pairs we know there is an edge\n",
      "between them, for some others we know there is no edge, and for the remaining\n",
      "we do not know whether or not there is an edge. We want to organize the nodes\n",
      "into disjoint clusters so that there is relatively dense (observed)\n",
      "connectivity within clusters, and sparse across clusters.\n",
      "  We take a novel yet natural approach to this problem, by focusing on finding\n",
      "the clustering that minimizes the number of \"disagreements\"---i.e., the sum of\n",
      "the number of (observed) missing edges within clusters, and (observed) present\n",
      "edges across clusters. Our algorithm uses convex optimization; its basis is a\n",
      "reduction of disagreement minimization to the problem of recovering an\n",
      "(unknown) low-rank matrix and an (unknown) sparse matrix from their partially\n",
      "observed sum. We evaluate the performance of our algorithm on the classical\n",
      "Planted Partition/Stochastic Block Model. Our main theorem provides sufficient\n",
      "conditions for the success of our algorithm as a function of the minimum\n",
      "cluster size, edge density and observation probability; in particular, the\n",
      "results characterize the tradeoff between the observation probability and the\n",
      "edge density gap. When there are a constant number of clusters of equal size,\n",
      "our results are optimal up to logarithmic factors.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(*papers_df.iloc[30], sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts = papers_df.apply(\n",
    "    lambda r: data_load.clean_description(r['title'] + ' ' + r['abstract']), axis=1\n",
    ").tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How to cluster a partially observed unweighted graph?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "priority_papers = paper_priority.PriorityPapersManager(contexts[:100], papers_df[\"id\"].iloc[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = priority_papers.merged_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_inputs = tokenizer(\n",
    "    question,\n",
    "    context,\n",
    "    padding=True,\n",
    "    truncation=\"only_second\",\n",
    "    max_length=512,\n",
    "    stride=100,\n",
    "    return_token_type_ids=True,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    "    return_special_tokens_mask=True,\n",
    "    return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_spans = len(encoded_inputs[\"input_ids\"])\n",
    "\n",
    "p_mask = [\n",
    "    [tok != 1 for tok in encoded_inputs.sequence_ids(span_id)]\n",
    "    for span_id in range(num_spans)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask', 'offset_mapping', 'overflow_to_sample_mapping'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens_mask = encoded_inputs[\"special_tokens_mask\"]\n",
    "offset_mapping = encoded_inputs[\"offset_mapping\"]\n",
    "overflow_to_sample_mapping = encoded_inputs[\"overflow_to_sample_mapping\"]\n",
    "\n",
    "del encoded_inputs[\"special_tokens_mask\"]\n",
    "del encoded_inputs[\"offset_mapping\"]\n",
    "del encoded_inputs[\"overflow_to_sample_mapping\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_scores = model(**encoded_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import softmax\n",
    "\n",
    "confidence_start = torch.max(softmax(answer_start_scores.flatten(), dim=0))\n",
    "confidence_end = torch.max(softmax(answer_end_scores.flatten(), dim=0))\n",
    "confidence_score = confidence_start * confidence_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00028420527814887464"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(confidence_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How to cluster a partially observed unweighted graph?\n",
      "Answer: convex optimization\n",
      "\n"
     ]
    }
   ],
   "source": [
    "answer_start = torch.argmax(\n",
    "    answer_start_scores\n",
    ")  # Get the most likely beginning of answer with the argmax of the score\n",
    "answer_end = torch.argmax(answer_end_scores) + 1  # Get the most likely end of answer with the argmax of the score\n",
    "\n",
    "answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(tokenized_example[\"input_ids\"].flatten()[answer_start:answer_end]))\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.bert.tokenization_bert_fast.BertTokenizerFast"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.5788, grad_fn=<DivBackward0>)\n",
      "tensor(3.5331, grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "confidence_start = torch.max(answer_start_scores)\n",
    "confidence_end = torch.max(answer_end_scores)\n",
    "\n",
    "print((confidence_start + confidence_end) / 2)\n",
    "print((confidence_start * confidence_end) ** 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_context.find_document(int(answer_start), int(answer_end))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to get multiple answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_start_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_answers_start = torch.topk(answer_start_scores.flatten(), 3).indices\n",
    "top_k_answers_end = torch.topk(answer_end_scores.flatten(), 3).indices + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = []\n",
    "document_idxs = []\n",
    "for start, end in zip(top_k_answers_start, top_k_answers_end):\n",
    "    answers.append(tokenizer.convert_ids_to_tokens(tokenized_example[\"input_ids\"].flatten()[start:end]))\n",
    "    document_idxs.append(merged_context.find_document(int(start), int(end)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = tokenization.qa_pre_tokenize_text([question, question])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "708c9295d9724058ac2356c20a9bf8e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/473 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4531de804344a71859ce2c42ed593e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/261M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ee781ec18334d62bfbe8987506112e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fdc7d0e3bd34b20913b09c5a22fc341",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fc7f38e454449bb834d2887ef59fdb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = pipeline(\n",
    "    task=\"question-answering\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = model(\n",
    "    {\n",
    "        \"question\": \"How to use gaussian processes?\",\n",
    "        \"context\": priority_papers.merged_context\n",
    "    },\n",
    "    top_k=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.8984388113021851, 'start': 63094, 'end': 63119, 'answer': 'kernel density estimation'}\n",
      "57    Sparse Nonparametric Graphical Models\n",
      "Name: title, dtype: object\n",
      "{'score': 0.7347434759140015, 'start': 95415, 'end': 95484, 'answer': 'alternates between clustering and training discriminative classifiers'}\n",
      "87    Unsupervised Discovery of Mid-Level Discriminative Patches\n",
      "Name: title, dtype: object\n",
      "{'score': 0.6878352165222168, 'start': 95415, 'end': 95484, 'answer': 'alternates between clustering and training discriminative classifiers'}\n",
      "87    Unsupervised Discovery of Mid-Level Discriminative Patches\n",
      "Name: title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "for answer in answers:\n",
    "    start = answer[\"start\"]\n",
    "    end = answer[\"end\"]\n",
    "    text = answer[\"answer\"]\n",
    "    print(answer)\n",
    "    paper_id= priority_papers.find_paper(start, end)\n",
    "    print(papers_df.loc[papers_df[\"id\"]==paper_id, \"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1205.3137', 87)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "priority_papers.find_paper(start, end, return_paper_idx=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' evolutionary inference for function valued traits gaussian process regression on phylogenies biological data objects often have both of the following features i they are functions rather than single numbers or vectors and ii they are correlated due to phylogenetic relationships in this paper we give a flexible statistical model for such data by combining assumptions from phylogenetics with gaussian processes we describe its use as a nonparametric bayesian prior distribution both for prediction placing posterior distributions on ancestral functions and model selection comparing rates of evolution across a phylogeny or identifying the most likely phylogenies consistent with the observed data our work is integrative extending the popular phylogenetic brownian motion and ornstein uhlenbeck models to functional data and bayesian inference and extending gaussian process regression to phylogenies we provide a brief illustration of the application of our method '"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contexts[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' sparse nonparametric graphical models we present some nonparametric methods for graphical modeling in the discrete case where the data are binary or drawn from a finite alphabet markov random fields are already essentially nonparametric since the cliques can take only a finite number of values continuous data are different the gaussian graphical model is the standard parametric model for continuous data but it makes distributional assumptions that are often unrealistic we discuss two approaches to building more flexible graphical models one allows arbitrary graphs and a nonparametric extension of the gaussian the other uses kernel density estimation and restricts the graphs to trees and forests examples of both methods are presented we also discuss possible future research directions for nonparametric graphical modeling '"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contexts[57]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "5 validation errors for Settings\nredis_host\n  field required (type=value_error.missing)\nredis_port\n  field required (type=value_error.missing)\nredis_db\n  field required (type=value_error.missing)\nredis_password\n  field required (type=value_error.missing)\ndata_location\n  field required (type=value_error.missing)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_26476/664682705.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mredis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masyncio\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mredis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_settings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;31m# search_index = SearchIndex()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# redis_client = redis.from_url(config.get_redis_url)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/redis-team-THM/backend/thm/config/settings.py\u001b[0m in \u001b[0;36mget_settings\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_settings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mSettings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_env_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"../backend/thm/config/prod.env\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0;31m# contact Michel for the gitignored file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/conda/envs/saturn/lib/python3.7/site-packages/pydantic/env_settings.cpython-37m-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mpydantic.env_settings.BaseSettings.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/srv/conda/envs/saturn/lib/python3.7/site-packages/pydantic/main.cpython-37m-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValidationError\u001b[0m: 5 validation errors for Settings\nredis_host\n  field required (type=value_error.missing)\nredis_port\n  field required (type=value_error.missing)\nredis_db\n  field required (type=value_error.missing)\nredis_password\n  field required (type=value_error.missing)\ndata_location\n  field required (type=value_error.missing)"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "sys.path.append(\"../backend\")\n",
    "\n",
    "from thm.search_index import SearchIndex\n",
    "from thm.config.settings import get_settings\n",
    "import redis.asyncio as redis\n",
    "\n",
    "config = get_settings()\n",
    "# search_index = SearchIndex()\n",
    "# redis_client = redis.from_url(config.get_redis_url)\n",
    "\n",
    "# # Create query\n",
    "# query = search_index.vector_query(\n",
    "#     similarity_request.categories,\n",
    "#     similarity_request.years,\n",
    "#     similarity_request.search_type,\n",
    "#     similarity_request.number_of_results,\n",
    "# )\n",
    "# count_query = search_index.count_query(\n",
    "#     years=similarity_request.years, categories=similarity_request.categories\n",
    "# )\n",
    "\n",
    "# # obtain results of the queries\n",
    "# total, results = await asyncio.gather(\n",
    "#     redis_client.ft(config.index_name).search(count_query),\n",
    "#     redis_client.ft(config.index_name).search(\n",
    "#         query,\n",
    "#         query_params={\n",
    "#             \"vec_param\": embeddings.make(similarity_request.user_text).tobytes()\n",
    "#         },\n",
    "#     ),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "5 validation errors for Settings\nredis_host\n  field required (type=value_error.missing)\nredis_port\n  field required (type=value_error.missing)\nredis_db\n  field required (type=value_error.missing)\nredis_password\n  field required (type=value_error.missing)\ndata_location\n  field required (type=value_error.missing)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_26476/3895121150.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_settings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/workspace/redis-team-THM/backend/thm/config/settings.py\u001b[0m in \u001b[0;36mget_settings\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_settings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mSettings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_env_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"../backend/thm/config/prod.env\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0;31m# contact Michel for the gitignored file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/conda/envs/saturn/lib/python3.7/site-packages/pydantic/env_settings.cpython-37m-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mpydantic.env_settings.BaseSettings.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/srv/conda/envs/saturn/lib/python3.7/site-packages/pydantic/main.cpython-37m-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValidationError\u001b[0m: 5 validation errors for Settings\nredis_host\n  field required (type=value_error.missing)\nredis_port\n  field required (type=value_error.missing)\nredis_db\n  field required (type=value_error.missing)\nredis_password\n  field required (type=value_error.missing)\ndata_location\n  field required (type=value_error.missing)"
     ]
    }
   ],
   "source": [
    "get_settings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('saturn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3c1f51487d8a055fea90b37fa3b43d5a61376641bd8f503a1f79d6e81aa7dcb1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
